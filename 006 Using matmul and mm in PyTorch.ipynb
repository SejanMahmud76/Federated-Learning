{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95a09b2-e338-4279-a6cb-df1673cbbf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6df7799-eca5-4c8a-8924-6d5fa2196c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.randn(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16edf3e4-1a9f-4a3b-9abd-4df440b72784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9820,  0.7372, -0.5066, -1.0869,  1.3026, -0.6646,  0.2715, -1.1534,\n",
       "          0.9123,  0.9513]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66344db7-6de5-4d01-9e4c-df3574a813b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn_like(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2eac329-ac27-43c9-b4b3-888f82adefcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1387,  0.8330,  0.4120,  0.5661,  1.0890, -0.7317, -0.6370,  0.8847,\n",
       "          1.8705, -0.6116]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3380112-013e-4586-90b5-182cc59e29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = torch.randn(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdde37b9-55f6-46f3-983f-50e585ddbf54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3780]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af70fb1-c44c-4a68-9a2f-6acbc36051c9",
   "metadata": {},
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5538c172-30b8-4a9b-bed7-0bcd037c38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output =  torch.sum(input_img*weights)+bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6166ce6-9f8a-4ca8-9d11-4fd13d307732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7471]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c62d15-a9c0-405c-8109-3377461422e4",
   "metadata": {},
   "source": [
    "# Matrix Multiplication\n",
    "we can do the above calculation using mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "564c35b8-d5b7-4fcc-aada-4af3577661e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 1x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m  \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mbias\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 1x10)"
     ]
    }
   ],
   "source": [
    "output =  torch.mm(input_img,weights)+bias "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f924a5-b875-4353-9fae-daba540ea7a7",
   "metadata": {},
   "source": [
    "# for matrix multiplication mat1 and mat2 shapes must be (1x10 and 10x1)\n",
    "so to get rid of the error we have to reshape input_img or weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5715f8cd-2189-4829-995c-e7a4fb6983f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1417597070.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    output =  torch.mm(input_img,weights.view(10,1)+bias\u001b[0m\n\u001b[1;37m                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "output =  torch.mm(input_img,weights.view(10,1)+bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf89ccb-9ec5-4ef9-a46a-b4c7b0369132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
